{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10a034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.set_theme()\n",
    "#sns.set_palette(\"Greens_d\")\n",
    "#sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "#sns.color_palette(\"Greens_d\")\n",
    "\n",
    "#import plotly.express as px\n",
    "import tarfile\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom helper libraries\n",
    "import os\n",
    "#from os import listdir, path\n",
    "#from os.path import isfile, join, splitext\n",
    "\n",
    "import sys\n",
    "#import data.helpers as data_helpers\n",
    "#import visualization.helpers as viz_helpers\n",
    "\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "\n",
    "#from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6464d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "def Test_Imported_Functions():\n",
    "    print(\"Functions have been properly imported !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364802fc",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0ccd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nrows_data(DATA_URL, nrows, DATASET_COLUMNS):\n",
    "    data_loaded = pd.read_csv(DATA_URL, nrows=nrows, names=DATASET_COLUMNS)\n",
    "    return (data_loaded)\n",
    "\n",
    "def load_all_data(DATA_URL, DATASET_COLUMNS):\n",
    "    data_loaded = pd.read_csv(DATA_URL, names=DATASET_COLUMNS)\n",
    "    return (data_loaded)\n",
    "\n",
    "def load_formatted_data(DATA_URL, DATASET_ENCODING, DATASET_COLUMNS):\n",
    "    data_loaded = pd.read_csv(DATA_URL, encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "    return (data_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90813b08",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16582c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data): \n",
    "    #Remove rows where important information are missing:\n",
    "    data_cleaned = data.dropna(axis = 0, how='all')\n",
    "    #Clean duplicates\n",
    "    data_cleaned = data_cleaned.drop_duplicates()\n",
    "    #Change content in lowercase\n",
    "    data_cleaned = data_cleaned.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x)\n",
    "    #Filter order_dataset\n",
    "    return (data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae524af5",
   "metadata": {},
   "source": [
    "## Plot Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f188f69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615659bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors_from_values_integer(values, palette_name):\n",
    "    # normalize the values to range [0, 1]\n",
    "    normalized = (values - min(values)) / (max(values) - min(values))\n",
    "    # convert to indices\n",
    "    indices = np.round(normalized * (len(values) - 1)).astype(np.int32)\n",
    "    # use the indices to get the colors\n",
    "    palette = sns.color_palette(palette_name, len(values))\n",
    "    return np.array(palette).take(indices, axis=0)\n",
    "\n",
    "# def colors_from_values(values, palette_name):\n",
    "#     pal = sns.color_palette(palette_name, len(values))\n",
    "#     rank = values.argsort().argsort()   # http://stackoverflow.com/a/6266510/1628638\n",
    "#     palette=np.array(pal[::-1])[rank]\n",
    "#     return (palette)\n",
    "\n",
    "def colors_from_values_float(values: pd.Series, palette_name:str, ascending=True):\n",
    "    # convert to indices\n",
    "    values = values.sort_values(ascending=True).reset_index()\n",
    "    indices = values.sort_values(by=values.columns[0]).index\n",
    "    # use the indices to get the colors\n",
    "    palette = sns.color_palette(palette_name, len(values))\n",
    "    return np.array(palette).take(indices, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5b52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot Fill ratio in specified columns\n",
    "def plot_fill_ratio(data: pd.DataFrame, colunms_selected: list):\n",
    "    data_fill_ratio = pd.DataFrame(columns=['column_name', 'null_count', 'notnull_count'])\n",
    "    data_fill_ratio.drop(data_fill_ratio.index, inplace=True)        \n",
    "    for col in colunms_selected: \n",
    "        null_count = data[col].isna().sum()\n",
    "        notnull_count = data[col].notna().sum()\n",
    "        new_row = pd.DataFrame({'column_name':[col], 'null_count':[null_count], 'notnull_count':[notnull_count]})\n",
    "        data_fill_ratio = pd.concat([data_fill_ratio, new_row], ignore_index = None, axis = 0)\n",
    "    data_fill_ratio_study = pd.melt(data_fill_ratio.reset_index(), id_vars=['column_name'], value_vars=['null_count', 'notnull_count'])\n",
    "    fig, ax = plt.subplots(figsize=(16,10))\n",
    "    #ax = sns.barplot(data=data_fill_ratio_study, x='value', y='column_name', hue='variable')\n",
    "    \n",
    "    ax = sns.barplot(data=data_fill_ratio_study, x='value', y='column_name', hue='variable', palette=\"Greens_d\")\n",
    "    ax.set_title('Null and NotNull Count per columns in dataframe')\n",
    "    plt.show()\n",
    "    data_fill_ratio_study.drop(data_fill_ratio_study.index, inplace=True)\n",
    "    return(data_fill_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d869e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot occurence by value present in specified column\n",
    "def plot_occurence_line(data: pd.DataFrame, colunm_name):\n",
    "    fig = px.line(data[colunm_name].value_counts())\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Number of occurence by {colunm_name} .\\nTOTAL = {len(data[colunm_name])}\",\n",
    "        width=900,\n",
    "        height=600,\n",
    "        #markers=True,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b2cbcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot distribution of dates\n",
    "def plot_peryearmonth(data: pd.DataFrame, date_column, plot_hue: bool, hue_column):\n",
    "    data['date_yearmonth'] = pd.to_datetime(data[date_column]).dt.to_period('M')\n",
    "    plt.figure(figsize=(15,10))\n",
    "    if (plot_hue == True):\n",
    "        ax1 = sns.countplot(x=\"date_yearmonth\", data=data.sort_values('date_yearmonth'), hue=hue_column, palette=\"Greens_d\")\n",
    "    else:\n",
    "        ax1 = sns.countplot(x=\"date_yearmonth\", data=data.sort_values('date_yearmonth'), palette=\"Greens_d\")\n",
    "    ax1.set_title(f'Distribution of {date_column} per month')\n",
    "    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9921ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_columns(data: pd.DataFrame, colunms_selected: list, plot_hue: bool, hue_column):\n",
    "    #fig, ax = plt.subplots(figsize=(15,10))\n",
    "    if (plot_hue == True):\n",
    "        ax = sns.pairplot(data[colunms_selected], \n",
    "                             hue=hue_column, \n",
    "                             hue_order=sorted(data[hue_column].unique(),\n",
    "                             reverse=True)\n",
    "                            )\n",
    "    else:\n",
    "        ax = ax=sns.pairplot(data[colunms_selected]\n",
    "                            )\n",
    "    ax.fig.suptitle(f'Pairplot on selected columns')\n",
    "    plt.title(f'Pairplot on selected columns {colunms_selected}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f964220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot PIE Chart of n tops values in dataframe\n",
    "def plot_ntops_pie(data: pd.DataFrame, colunm_name, ntops: int, plot_others: bool, plot_na: bool):\n",
    "    podium_tops = pd.DataFrame(data[colunm_name].value_counts(dropna=True, sort=True).head(ntops))\n",
    "    if (plot_others == True):\n",
    "        remainings_counts = sum(data[colunm_name].value_counts(dropna=True)[ntops:])\n",
    "        remainings_below = pd.DataFrame({colunm_name : [remainings_counts]}, index=['others'])\n",
    "        podium_tops = pd.concat([podium_tops, remainings_below], ignore_index = None, axis = 0)\n",
    "    if (plot_na == True):\n",
    "        na_counts = data[colunm_name].isna().sum()\n",
    "        remainings_na = pd.DataFrame({colunm_name : [na_counts]}, index=['NAN'])\n",
    "        podium_tops = pd.concat([podium_tops, remainings_na], ignore_index = None, axis = 0)\n",
    "    \n",
    "    \n",
    "    #DÃ©finir la taille du graphique\n",
    "    plt.figure(figsize=(8,8))\n",
    "    #DÃ©finir lae type du graphique, ici PIE CHart avec en Labels l'index du nom des libelle\n",
    "    #l'autopct sert ici Ã  afficher le % calculÃ© avec 1 dÃ©cimal derriere la virgule\n",
    "    plt.pie(podium_tops[colunm_name], labels=podium_tops.index, autopct='%1.1f%%')\n",
    "    #Afficher la lÃ©gende en dessous du graphique au centre\n",
    "    plt.legend(loc='upper left', bbox_to_anchor=(0.1, -0.01), fancybox=True, shadow=None, ncol=2)\n",
    "    plt.title(f\"{ntops} most presents values identified in column {colunm_name} .\\nTOTAL unique = {len(data[colunm_name].unique())}\")\n",
    "    #Afficher le graphique\n",
    "    plt.show()\n",
    "    return(podium_tops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3e01567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ntops_bar(data: pd.DataFrame, colunm_name, ntops: int, plot_others: bool, plot_na: bool):\n",
    "    podium_tops = pd.DataFrame(data[colunm_name].value_counts(dropna=True, sort=True).head(ntops))\n",
    "    if (plot_others == True):\n",
    "        remainings_counts = sum(data[colunm_name].value_counts(dropna=True)[ntops:])\n",
    "        remainings_below = pd.DataFrame({colunm_name : [remainings_counts]}, index=['others'])\n",
    "        podium_tops = pd.concat([podium_tops, remainings_below], ignore_index = None, axis = 0)\n",
    "    if (plot_na == True):\n",
    "        na_counts = data[colunm_name].isna().sum()\n",
    "        remainings_na = pd.DataFrame({colunm_name : [na_counts]}, index=['NAN'])\n",
    "        podium_tops = pd.concat([podium_tops, remainings_na], ignore_index = None, axis = 0)\n",
    "    #podium_tops = podium_tops.reset_index(drop=True)\n",
    "    #DÃ©finir la taille du graphique\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    #DÃ©finir lae type du graphique, ici BARPLOT avec en Labels l'index du nom des libelle\n",
    "    ax = sns.barplot(data=podium_tops, x=podium_tops.index, y=colunm_name, palette=colors_from_values_integer(podium_tops[colunm_name], \"Greens_d\"))\n",
    "    plt.title(f\"{ntops} most presents values identified in column {colunm_name} .\\nTOTAL unique = {len(data[colunm_name].unique())}\")\n",
    "    #Afficher le graphique\n",
    "    plt.show()\n",
    "    return(podium_tops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c0ab79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that study boxplot\n",
    "def plot_boxplot(data: pd.DataFrame, x_axis, colunms_selected: list, plot_outliers: bool): \n",
    "    for col in colunms_selected:\n",
    "        sns.set()\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        sns.boxplot(x=x_axis, \n",
    "                    y=col, # column is chosen here\n",
    "                    data=data,\n",
    "                    #order=[\"a\", \"b\"],\n",
    "                    showfliers = plot_outliers,\n",
    "                    showmeans=True,\n",
    "                    )  \n",
    "        sns.despine(offset=10, trim=True) \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9261b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function that study histogramme\n",
    "def plot_histogramme(histo_data: pd.DataFrame, column_value, colunms_group, plot_outliers: bool): \n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "    #Plot the distribution\n",
    "    ax = sns.displot(data=histo_data, x=column_value, hue=colunms_group)\n",
    "    ax.move_legend(ax1, \"upper right\", bbox_to_anchor=(.55, .45), title=f'histogramme of {column_value}')\n",
    "    plt.title(f\"Distribution of {column_value} values\")\n",
    "    #plt.legend(loc='upper right')\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(f\"{column_value} ranges\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0415f5d2",
   "metadata": {},
   "source": [
    "## RÃ©duction de dimension\n",
    "### ACP (A VÃ©rifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e43b7ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA functions:\n",
    "#Functions below are used for ACP\n",
    "def display_circles(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n",
    "    for d1, d2 in axis_ranks: # On affiche les 3 premiers plans factoriels, donc les 6 premiÃ¨res composantes\n",
    "        if d2 < n_comp:\n",
    "\n",
    "            # initialisation de la figure\n",
    "            fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "            # dÃ©termination des limites du graphique\n",
    "            if lims is not None :\n",
    "                xmin, xmax, ymin, ymax = lims\n",
    "            elif pcs.shape[1] < 30 :\n",
    "                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n",
    "            else :\n",
    "                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n",
    "\n",
    "            # affichage des flÃ¨ches\n",
    "            # s'il y a plus de 30 flÃ¨ches, on n'affiche pas le triangle Ã  leur extrÃ©mitÃ©\n",
    "            if pcs.shape[1] < 30 :\n",
    "                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "                   pcs[d1,:], pcs[d2,:], \n",
    "                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n",
    "                # (voir la doc : https://matplotlib.org/api/_as_gen/matplotlib.pyplot.quiver.html)\n",
    "            else:\n",
    "                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n",
    "                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n",
    "            \n",
    "            # affichage des noms des variables  \n",
    "            if labels is not None:  \n",
    "                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n",
    "                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n",
    "                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n",
    "            \n",
    "            # affichage du cercle\n",
    "            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='r')\n",
    "            plt.gca().add_artist(circle)\n",
    "\n",
    "            # dÃ©finition des limites du graphique\n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.ylim(ymin, ymax)\n",
    "        \n",
    "            # affichage des lignes horizontales et verticales\n",
    "            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "            # nom des axes, avec le pourcentage d'inertie expliquÃ©\n",
    "            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Cercle des corrÃ©lations (F{} et F{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed0397e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions below are used for ACP\n",
    "def display_factorial_planes(X_projected, n_comp, pca, axis_ranks, labels=None, alpha=1, illustrative_var=None):\n",
    "    for d1,d2 in axis_ranks:\n",
    "        if d2 < n_comp:\n",
    " \n",
    "            # initialisation de la figure       \n",
    "            fig = plt.figure(figsize=(8,8))\n",
    "        \n",
    "            # affichage des points\n",
    "            if illustrative_var is None:\n",
    "                plt.scatter(X_projected[:, d1], X_projected[:, d2], alpha=alpha)\n",
    "                #plt.scatter(centres_reduced[:, d1], centres_reduced[:, d2], alpha=alpha, marker='x', s=100, linewidths=2,color='k', zorder=10)\n",
    "            else:\n",
    "                illustrative_var = np.array(illustrative_var)\n",
    "                for value in np.unique(illustrative_var):\n",
    "                    selected = np.where(illustrative_var == value)\n",
    "                    plt.scatter(X_projected[selected, d1], X_projected[selected, d2], alpha=alpha, label=value)\n",
    "                    #plt.scatter(centres_reduced[:, d1], centres_reduced[:, d2], alpha=alpha, marker='x', s=100, linewidths=2,color='k', zorder=10)\n",
    "                plt.legend()\n",
    "\n",
    "            # affichage des labels des points\n",
    "            if labels is not None:\n",
    "                for i,(x,y) in enumerate(X_projected[:,[d1,d2]]):\n",
    "                    plt.text(x, y, labels[i],\n",
    "                              fontsize='14', ha='center',va='center') \n",
    "                \n",
    "            # dÃ©termination des limites du graphique\n",
    "            boundary = np.max(np.abs(X_projected[:, [d1,d2]])) / 1\n",
    "            plt.xlim([-boundary,boundary])\n",
    "            plt.ylim([-boundary,boundary])\n",
    "        \n",
    "            # affichage des lignes horizontales et verticales\n",
    "            plt.plot([-100, 100], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-100, 100], color='grey', ls='--')\n",
    "\n",
    "            # nom des axes, avec le pourcentage d'inertie expliquÃ©\n",
    "            plt.xlabel('F{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('F{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Projection des individus (sur F{} et F{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9a6a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions below are used for ACP            \n",
    "def display_scree_plot(pca):\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "    plt.xlabel(\"rang de l'axe d'inertie\")\n",
    "    plt.ylabel(\"pourcentage d'inertie\")\n",
    "    plt.title(\"Eboulis des valeurs propres\")\n",
    "    plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a8287e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_parallel_coordinates(df, num_clusters):\n",
    "    '''Display a parallel coordinates plot for the clusters in df'''\n",
    "\n",
    "    # Select data points for individual clusters\n",
    "    cluster_points = []\n",
    "    for i in range(num_clusters):\n",
    "        cluster_points.append(df[df.cluster==i])\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(12, 15))\n",
    "    title = fig.suptitle(\"Parallel Coordinates Plot for the Clusters\", fontsize=18)\n",
    "    fig.subplots_adjust(top=0.95, wspace=0)\n",
    "\n",
    "    # Display one plot for each cluster, with the lines for the main cluster appearing over the lines for the other clusters\n",
    "    for i in range(num_clusters):    \n",
    "        plt.subplot(num_clusters, 1, i+1)\n",
    "        for j,c in enumerate(cluster_points): \n",
    "            if i!= j:\n",
    "                pc = parallel_coordinates(c, 'cluster', color=[addAlpha(palette[j],0.2)])\n",
    "        pc = parallel_coordinates(cluster_points[i], 'cluster', color=[addAlpha(palette[i],0.5)])\n",
    "\n",
    "        # Stagger the axes\n",
    "        ax=plt.gca()\n",
    "        for tick in ax.xaxis.get_major_ticks()[1::2]:\n",
    "            tick.set_pad(20)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b9fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_parallel_coordinates_centroids(df, num_clusters):\n",
    "    '''Display a parallel coordinates plot for the centroids in df'''\n",
    "\n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    title = fig.suptitle(\"Parallel Coordinates plot for the Centroids\", fontsize=18)\n",
    "    fig.subplots_adjust(top=0.9, wspace=0)\n",
    "\n",
    "    # Draw the chart\n",
    "    parallel_coordinates(df, 'cluster', color=palette)\n",
    "\n",
    "    # Stagger the axes\n",
    "    ax=plt.gca()\n",
    "    for tick in ax.xaxis.get_major_ticks()[1::2]:\n",
    "        tick.set_pad(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d6a1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions below are used for Data Clustering    \n",
    "def plot_dendrogram(linked, names):\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "    plt.xlabel('distance')\n",
    "    dendrogram(\n",
    "        linked,\n",
    "        labels = names,\n",
    "        orientation = \"left\",\n",
    "        show_leaf_counts=True\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81dd222",
   "metadata": {},
   "source": [
    "### Matrice de Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb76607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction pour le graphe de Matrice de Confusion\n",
    "def matrix_pred_model(model, model_name, y_test, y_pred, figsize=(5,5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(f\"Matrice de confusion de {model_name}\")\n",
    "    sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap = 'Greens',fmt=\"d\",cbar=False)\n",
    "    plt.xlabel(\"Classe prÃ©dite\")\n",
    "    plt.ylabel(\"Classe initiale\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de8b9a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_auc_curve(model_name, fpr, tpr, figsize=(5,5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(f\"ROC Curve for {model_name}\")\n",
    "    plt.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "    plt.plot(fpr, tpr, marker='.', label=model_name)\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f670bc",
   "metadata": {},
   "source": [
    "### Determiner le seuil de dÃ©cision (Classification binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold_scores(data: pd.DataFrame, optimized_seuil):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(data['seuil'], data['FBeta-Score'], color='coral', lw=2, label='FBeta-Score')\n",
    "    plt.plot(data['seuil'], data['Precision_score'], color='cyan', lw=2, label='Precision_score')\n",
    "    plt.plot(data['seuil'], data['Accuracy_score'], color='blue', lw=2, label='Accuracy_score')\n",
    "    plt.plot(data['seuil'], data['Recall_score'], color='green', lw=2, label='Recall_score')\n",
    "    plt.plot(data['seuil'], data['F1_score'], color='red', lw=2, label='F1_score')\n",
    "    #plt.plot(store_score_thresholds['seuil'], store_score_thresholds['Roc_AUC_score'], color='red', lw=2, label='Roc_AUC_score')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Seuil', fontsize=14)\n",
    "    plt.ylabel('Scores', fontsize=14)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(f'Score optimized for binary classification obtained with threshold = {optimized_seuil}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a26ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentiment(score, seuil=0.5):\n",
    "    if score <= seuil:\n",
    "        label = 0\n",
    "    elif score > seuil:\n",
    "        label = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785f7294",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3091174520.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    F1_score = f1_score(y_test, y_pred) #, pos_label=4\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Fonction pour retourner les score des modÃ¨les\n",
    "\n",
    "###############################        ATTENTION : INITIALISATION A PREVOIR AVANT UTILISATION     ################\n",
    "#Initialisation de la table des rÃ©sultats\n",
    "#score_column_names = [\"Model Type\",\"Model Name\",\"seuil\",\"F1-Score\", \"Recall_score\", \"Precision_score\", \"Accuracy_score\"]\n",
    "#store_score= pd.DataFrame(columns = score_column_names)\n",
    "\n",
    "# def evaluation(model,model_name,score_column_names,X_test,y_test, seuil = 0.5, binary_transform=False):\n",
    "#     # On rÃ©cupÃ¨re la prÃ©diction de la valeur positive\n",
    "#     if binary_transform == True:\n",
    "#         y_prob = model.predict(X_test)\n",
    "#         y_pred = y_prob\n",
    "#     else:\n",
    "#         y_prob = model.predict_proba(X_test)[:,1]\n",
    "#         y_pred = np.where(y_prob > seuil, 1, 0)\n",
    "    \n",
    "#     # On crÃ©Ã© un vecteur de prÃ©diction Ã  partir du vecteur de probabilitÃ©s\n",
    "#     false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob) # y_prob instead of y_prob #, pos_label=4\n",
    "#     Roc_AUC_score = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "#     F1_score = f1_score(y_test, y_pred)\n",
    "#     FBeta_score = fbeta_score(y_test, y_pred, average='binary', beta=0.5, pos_label=1) #make_scorer(fbeta_score, beta = 2, pos_label=0 ,average = 'binary')\n",
    "#     Recall_score = recall_score(y_test, y_pred)\n",
    "#     Precision_score = precision_score(y_test, y_pred)\n",
    "#     Accuracy_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "#     #Plot functions\n",
    "#     matrix_pred_model(model, model_name, y_test, y_pred) \n",
    "#     plot_roc_auc_curve(model_name, false_positive_rate, true_positive_rate)\n",
    "    \n",
    "#     score_results = pd.Series([model, model_name, seuil, F1_score, FBeta_score, Recall_score, Precision_score, Accuracy_score, Roc_AUC_score])\n",
    "#     score_results_stored = pd.DataFrame([score_results.values],  columns = score_column_names)\n",
    "#     return(score_results_stored)\n",
    "\n",
    "# def evaluation_to_correct(model,model_name,score_column_names,store_score,X_test,y_test, seuil = 0.5):\n",
    "#     #Si le seuil n'est pas important\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     F1_score = f1_score(y_test, y_pred)#, pos_label=4\n",
    "#     Recall_score = recall_score(y_test, y_pred) #, pos_label=4\n",
    "#     Precision_score = precision_score(y_test, y_pred) #, pos_label=4\n",
    "#     Accuracy_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "#     matrix_pred_model(model, model_name, y_test,y_pred)  \n",
    "    \n",
    "#     #global store_score\n",
    "#     score_results = pd.Series([model, model_name, seuil, F1_score, Recall_score, Precision_score, Accuracy_score])\n",
    "#     score_results_stored = pd.DataFrame([score_results.values],  columns = score_column_names)\n",
    "#     store_score = pd.concat([store_score, score_results_stored], axis=0)\n",
    "#     return(store_score)\n",
    "\n",
    "def evaluation(model,model_name,score_column_names,X_test,y_test, seuil = 0.5, binary_predict=False, predict_proba_OK=False):\n",
    "    # On rÃ©cupÃ¨re la prÃ©diction de la valeur positive\n",
    "    if binary_predict == True:\n",
    "        y_prob = model.predict(X_test)\n",
    "        y_pred = y_prob\n",
    "    elif ((binary_predict == False) and (predict_proba_OK == True)):\n",
    "        y_prob = model.predict_proba(X_test)[:,1]\n",
    "        y_pred = np.where(y_prob > seuil, 1, 0)\n",
    "    elif predict_proba_OK == False:\n",
    "        y_prob = model.predict(X_test)\n",
    "        y_pred = np.where(y_prob > seuil, 1, 0)\n",
    "        y_pred = y_pred.astype(int)\n",
    "    \n",
    "    # On crÃ©Ã© un vecteur de prÃ©diction Ã  partir du vecteur de probabilitÃ©s\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_prob) # y_prob instead of y_prob #, pos_label=4\n",
    "    Roc_AUC_score = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    F1_score = f1_score(y_test, y_pred)\n",
    "    FBeta_score = fbeta_score(y_test, y_pred, average='binary', beta=0.5, pos_label=1) #make_scorer(fbeta_score, beta = 2, pos_label=0 ,average = 'binary')\n",
    "    Recall_score = recall_score(y_test, y_pred)\n",
    "    Precision_score = precision_score(y_test, y_pred)\n",
    "    Accuracy_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    #Plot functions\n",
    "    matrix_pred_model(model, model_name, y_test, y_pred) \n",
    "    plot_roc_auc_curve(model_name, false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    score_results = pd.Series([model, model_name, seuil, F1_score, FBeta_score, Recall_score, Precision_score, Accuracy_score, Roc_AUC_score])\n",
    "    score_results_stored = pd.DataFrame([score_results.values],  columns = score_column_names)\n",
    "    return(score_results_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ead5bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_result(data: pd.DataFrame, score, model_name):\n",
    "    #DÃ©finir la taille du graphique\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    #DÃ©finir lae type du graphique, ici BARPLOT avec en Labels l'index du nom des libelle\n",
    "    ax = sns.barplot(data=data, y=model_name, x=score, palette=colors_from_values_float(data[score], \"Greens_d\"))\n",
    "    ax.set_xlim((data[score].min() - 0.05), (data[score].max() + 0.02))\n",
    "    #ax = sns.barplot(data=data, x=model_name, y=score)\n",
    "    plt.title(f\"Score {score} max value is {round(data[score].max(), 2)} computed in model {data.loc[data[score] == data[score].max(), model_name]}\")\n",
    "    #Afficher le graphique\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7967f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc=history.history['accuracy']\n",
    "    val_acc=history.history['val_accuracy']\n",
    "    loss=history.history['loss']\n",
    "    val_loss=history.history['val_loss']\n",
    "    x=range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'g', label='Training accuracy')\n",
    "    plt.plot(x, val_acc, 'c', label='Validation accuracy')\n",
    "    plt.title('Training and Validation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'g', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'c', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_learning_curves(history):\n",
    "    acc = history.history[\"mean_io_u\"]\n",
    "    val_acc = history.history[\"val_mean_io_u\"]\n",
    "\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    epochs_range = range(30)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, acc, label=\"train mean_io_u\")\n",
    "    plt.plot(epochs_range, val_acc, label=\"validataion mean_io_u\")\n",
    "    plt.title(\"mean_io_u\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"mean_io_u\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, loss, label=\"train loss\")\n",
    "    plt.plot(epochs_range, val_loss, label=\"validataion loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_learning_curves_iou(history):\n",
    "    iou = history.history[\"iou_score\"]\n",
    "    val_iou = history.history[\"val_iou_score\"]\n",
    "\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs_range=range(1, len(iou) + 1)\n",
    "    #epochs_range = range(n_epochs)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, iou, label=\"train iou_score\")\n",
    "    plt.plot(epochs_range, val_iou, label=\"validataion iou_score\")\n",
    "    plt.title(\"iou_score\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"iou_score\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, loss, label=\"train loss\")\n",
    "    plt.plot(epochs_range, val_loss, label=\"validataion loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.suptitle(f\"IoU Socre and Loss evolution during {model_test.name} training, (stopped by callback at epochs : {len(iou)})  \")\n",
    "    #fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd7c54",
   "metadata": {},
   "source": [
    "## Specific For Image project 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(directory):\n",
    "    #From Data path, list folders and files and store information in dataframe\n",
    "    train_val_test = []\n",
    "    cities = []\n",
    "    data = []\n",
    "    for phase in sorted(os.listdir(directory)):\n",
    "        train_val_test.append((phase))\n",
    "        for city in sorted(os.listdir(os.path.join(directory, phase))):\n",
    "            cities.append((city))\n",
    "            for files in sorted(os.listdir(os.path.join(directory, phase, city))):\n",
    "                filename_path = os.path.join(directory, phase, city, files)\n",
    "                data.append((phase, city, files, filename_path))       \n",
    "    return (pd.DataFrame(data, columns=['Phase', 'City', 'File', 'Path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0659154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_filename(data: pd.DataFrame):\n",
    "    data['File_id1'] = data['File'].str.split('_').str[1]\n",
    "    data['File_id2'] = data['File'].str.split('_').str[2]\n",
    "    data['File_type'] = data['File'].str.split('_').str[3]\n",
    "    data['File_unique_index'] = data['City'] + '_' + data['File_id1'] + '_' + data['File_id2']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85803dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_images_masks(data_masks: pd.DataFrame, data_images: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Permet de lier les images et les masques dans un seul dataframe\n",
    "    :param data_images: DataFrame contenant les informations sur les images\n",
    "    :param data_masks: DataFrame contenant les informations sur les masks\n",
    "    :return: cleaned DataFrame\n",
    "    \"\"\"\n",
    "    #Keep only lanelIDs from masks\n",
    "    clean_data_masks = data_masks.loc[data_masks['File'].str.endswith('labelIds.png') == True, :].copy()\n",
    "    #clean_data_masks = data_masks.loc[(data_masks['File'].str.contains('labelIds') == True), :]\n",
    "\n",
    "    #Get Info from filename\n",
    "    clean_data_images = get_info_from_filename(data_images)\n",
    "    clean_data_masks = get_info_from_filename(clean_data_masks)\n",
    "\n",
    "    #Merge images and masks dataframes on File_id1 and File_id2\n",
    "    #data = pd.merge(clean_data_images, clean_data_masks, how='inner', on=['File_id1', 'File_id2'])\n",
    "    #data = pd.merge(clean_data_images, clean_data_masks, how='inner', on=['File_id1', 'File_id2', 'City', 'Phase'])\n",
    "    data = pd.merge(clean_data_images, clean_data_masks, how='inner', on=['File_unique_index', 'City', 'File_id1', 'File_id2', 'Phase'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0123b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(list):\n",
    "    return tuple(list)\n",
    "def count_color_rgb(image):\n",
    "    all_color = {}\n",
    "    for x in range(image.shape[0]):\n",
    "        for y in range(image.shape[1]):\n",
    "            color =  convert(image[x,y].tolist())\n",
    "            if not color in all_color.keys():\n",
    "                all_color[color] = 1\n",
    "            else:\n",
    "                all_color[color] += 1\n",
    "    return sorted(all_color.items())\n",
    "\n",
    "def get_image_labelid(path:str):\n",
    "    \"\"\"RÃ©cupÃ©re le mask parfaitement formatÃ©.\n",
    "    argument:\n",
    "    - path (type str): chemin d'accÃ¨s complet ou partiel de l'image \"_labelIds.png\"\n",
    "    \"\"\"\n",
    "    return convert_mask(cv2.imread(path,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e04f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = {\n",
    "\t'construction': [11, 12, 13, 14, 15, 16],\n",
    "\t'flat': [7, 8, 9, 10],\n",
    "\t'human': [24, 25],\n",
    "\t'nature': [21, 22],\n",
    "\t'object': [17, 18, 19, 20],\n",
    "\t'sky': [23],\n",
    "\t'vehicle': [26, 27, 28, 29, 30, 31, 32, 33, -1],\n",
    "\t'void': [0, 1, 2, 3, 4, 5, 6],\n",
    "}\n",
    "\n",
    "def convert_mask(img):\n",
    "    \"\"\"Cette mÃ©thode permet de convertir l'image '_labelids.png' du jeu de donnÃ©es de CityScapes.\n",
    "    La mÃ©thode permet de rÃ©cupÃ©rer l'image au format one_hot_encoder ou au format label_encoder.\n",
    "    arguments:\n",
    "    - img (type numpy.array): image du jeu de donnÃ©es CityScapes '...labelids.png' au format numpy  \n",
    "    return:\n",
    "    mask (type numpy.array) : mask pour la segmentation sÃ©mantique au format label encoder avec les 8 catÃ©gories principal (void, flat, construction, object, nature, sky, human, vehicle)\n",
    "    \"\"\"\n",
    "    #print(len(img.shape))\n",
    "    if len(img.shape) == 3:\n",
    "        img = np.squeeze(img[:, :, 0])\n",
    "    else:\n",
    "        img = np.squeeze(img)\n",
    "        #print(img.shape)\n",
    "    mask = np.zeros((img.shape[0], img.shape[1], 8), dtype=np.uint16)\n",
    "    print(mask.shape)\n",
    "    for i in range(-1, 34):\n",
    "        if i in cats['void']: \n",
    "            #Logical OR is applied to the elements of mask and labelIds_img. If mask.shape != labelIds_img.shape,.\n",
    "            mask[:, :, 0] = np.logical_or(mask[:, :, 0], (img == i))\n",
    "        elif i in cats['flat']:\n",
    "            mask[:, :, 1] = np.logical_or(mask[:, :, 1], (img == i))\n",
    "        elif i in cats['construction']:\n",
    "            mask[:, :, 2] = np.logical_or(mask[:, :, 2], (img == i))\n",
    "        elif i in cats['object']:\n",
    "            mask[:, :, 3] = np.logical_or(mask[:, :, 3], (img == i))\n",
    "        elif i in cats['nature']:\n",
    "            mask[:, :, 4] = np.logical_or(mask[:, :, 4], (img == i))\n",
    "        elif i in cats['sky']:\n",
    "            mask[:, :, 5] = np.logical_or(mask[:, :, 5], (img == i))\n",
    "        elif i in cats['human']:\n",
    "            mask[:, :, 6] = np.logical_or(mask[:, :, 6], (img == i))\n",
    "        elif i in cats['vehicle']:\n",
    "            mask[:, :, 7] = np.logical_or(mask[:, :, 7], (img == i))\n",
    "    return np.array(np.argmax(mask, axis=2), dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a03c1",
   "metadata": {},
   "source": [
    "# DataLoader_simple from dataset - Project 8 (Sans Conversion des Masques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79581cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER - Notre classe hÃ©rite de la classe Keras.utils.Sequence\n",
    "# Elle permet de crÃ©er un gÃ©nÃ©rateur de donnÃ©es\n",
    "# Cette classe parente vous assure dans le cas ou vous souhaitez utiliser du calcul parallÃ¨le avec vos threads, de garantir de parcourir une seule et unique fois vos donnÃ©es au cours dâune Ã©poch\n",
    "class Dataloader_simple(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset to build bacthes\n",
    "    Args:\n",
    "        dataset : dataframe listing data and paths\n",
    "        n_sample : to work with a reduced dataset\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle dataset each epoch.\n",
    "        resize: Boolean,  if `True` resize images and mask to same dimensions\n",
    "        resize_width & resize_height: New dimensions after resizing\n",
    "        display: display images when calling dataloader\n",
    "        phase: allow to set if dataloader shall filter on train, val or test images/masks\n",
    "        augmentation: Variable defining augmentation of image and mask\n",
    "        normalization: Boolean,  if `True` normalizes images and masks RGB values ; The range for each individual colour is 0-255\n",
    "    \"\"\"\n",
    "    # We provide one dataset, containing labelIds and \n",
    "    def __init__(self, dataset, n_sample, batch_size=1, shuffle=False, resize=None, resize_width=256, resize_height=128, display=None, phase='train', augmentation=None, normalization=None):\n",
    "        self.n_sample = n_sample\n",
    "        self.phase = phase\n",
    "        self.dataset = dataset.loc[dataset['Phase'] == self.phase, :].sample(self.n_sample) #ou dataset si on veut prendre tour le jeu de donnÃ©e\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.resize = resize\n",
    "        self.resize_width = resize_width\n",
    "        self.resize_height = resize_height\n",
    "        self.augmentation = augmentation\n",
    "        self.normalization = normalization\n",
    "        self.display = display\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # collect batch images and masks\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        #print('start barch is ', start)\n",
    "        #print('stop barch is ', stop)\n",
    "        images = []\n",
    "        masks = []\n",
    "        for j in range(start, stop):\n",
    "            # Store entire paths to image_file and mask_file variable\n",
    "            image_file = list(self.dataset['Path_x'])[j]\n",
    "            mask_file = list(self.dataset['Path_y'])[j]\n",
    "            unique_index_file = list(self.dataset['File_unique_index'])[j]\n",
    "\n",
    "            # OpenImages and masks using OpenCV ibrary, and convert in np array\n",
    "            image = cv2.imread(image_file)\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            mask = cv2.imread(mask_file)\n",
    "            #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # apply resize\n",
    "            if self.resize:\n",
    "                dim = (self.resize_width, self.resize_height)\n",
    "                original_height = image.shape[0]\n",
    "                original_width = image.shape[1]\n",
    "                original_channels = image.shape[2]\n",
    "                image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA) # resize image\n",
    "                mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)\n",
    "                new_height = image.shape[0]\n",
    "                new_width = image.shape[1]\n",
    "                new_channels = image.shape[2]\n",
    "                \n",
    "\n",
    "            # apply display\n",
    "            if self.display:\n",
    "                #Display Image and Mask\n",
    "                plt.figure(figsize=(25, 5))\n",
    "                plt.subplots_adjust(hspace=0.5)\n",
    "                plt.subplot(120 + 1 + 0)\n",
    "                plt.imshow(image)\n",
    "                plt.subplot(120 + 1 + 1)\n",
    "                plt.imshow(mask)\n",
    "                plt.grid(False)\n",
    "                plt.suptitle(f'Images and Masks {unique_index_file}')\n",
    "                plt.show()\n",
    "                if self.resize:\n",
    "                    print('original height was ', original_height, ' and new height is ', new_height)\n",
    "                    print('original width was ', original_width, ' and new width is ', new_width)\n",
    "                    print('original channels was ', original_channels, ' and new channels is ', new_channels)\n",
    "                    \n",
    "\n",
    "            # apply normalization : Normalizing RGB values ; The range for each individual colour is 0-255 (as 2^8 = 256 possibilities).\n",
    "            if self.normalization:\n",
    "                #normalizedImg = np.zeros((800, 800))\n",
    "                image = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX) #ou np.array(image/255, dtype='uint8')\n",
    "                #mask = np.array(mask/255, dtype='uint8') # cv2.normalize(mask, None, 0, 1, cv2.NORM_MINMAX) #mask/255\n",
    "            \n",
    "                # apply display\n",
    "                if self.display:\n",
    "                    print(f'----------------Normalized Image and Mask {j} from batch collected-----------------')\n",
    "                    #Display Image and Mask\n",
    "                    plt.figure(figsize=(25, 5))\n",
    "                    plt.subplots_adjust(hspace=0.5)\n",
    "                    plt.subplot(120 + 1 + 0)\n",
    "                    plt.imshow(image)\n",
    "                    plt.subplot(120 + 1 + 1)\n",
    "                    plt.imshow(mask)\n",
    "                    plt.grid(False)\n",
    "                    plt.suptitle(f'Normalized Images and Masks {unique_index_file}')\n",
    "                    plt.show()\n",
    "            \n",
    "            # apply augmentations\n",
    "            if ((self.augmentation is not None) & (self.phase == 'train')):\n",
    "                # Augment an image\n",
    "                sample = self.augmentation(image=image, mask=mask)\n",
    "                augmented_image, augmented_mask = sample['image'], sample['mask']\n",
    "                # apply display\n",
    "                if self.display:\n",
    "                    print(f'----------------Augmented Image and Mask {j} from batch collected-----------------')\n",
    "                    #Display Image and Mask\n",
    "                    plt.figure(figsize=(25, 5))\n",
    "                    plt.subplots_adjust(hspace=0.5)\n",
    "                    plt.subplot(120 + 1 + 0)\n",
    "                    plt.imshow(augmented_image)\n",
    "                    plt.subplot(120 + 1 + 1)\n",
    "                    plt.imshow(augmented_mask)\n",
    "                    plt.grid(False)\n",
    "                    plt.suptitle(f'Augmented Images and Masks {unique_index_file}')\n",
    "                    plt.show()\n",
    "                    images.append(augmented_image)\n",
    "                    masks.append(augmented_mask)\n",
    "                    \n",
    "            # append image and mask to batch\n",
    "            images.append(image)\n",
    "            masks.append(mask)   \n",
    "            print(f'---------------- Image and Mask {j} from batch collected in Data Generator-----------------')\n",
    "\n",
    "        # transpose list of lists\n",
    "        image_batch = np.stack(images, axis=0) #A confirmer si images ou image\n",
    "        mask_batch = np.stack(masks, axis=0) #A confirmer si masks ou mask\n",
    "\n",
    "        #Add 1 dimension to return image_batch and mask_batch in 4 dimensions (batch number, height, width, channels), avec channel = 1 pour les masques et =3 pour les images\n",
    "        if len(mask_batch.shape) == 3:\n",
    "            mask_batch = np.expand_dims(mask_batch, axis = 3)\n",
    "\n",
    "        #print('mask_batch shape is ', mask_batch.shape)\n",
    "        return image_batch, mask_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle dataset each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.dataset.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63360e",
   "metadata": {},
   "source": [
    "# DataLoader_advanced from dataset - Project 8 (Avec Conversion des Masques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER - Notre classe hÃ©rite de la classe Keras.utils.Sequence\n",
    "# Elle permet de crÃ©er un gÃ©nÃ©rateur de donnÃ©es\n",
    "# Cette classe parente vous assure dans le cas ou vous souhaitez utiliser du calcul parallÃ¨le avec vos threads, de garantir de parcourir une seule et unique fois vos donnÃ©es au cours dâune Ã©poch\n",
    "class Dataloader_advanced(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset to build bacthes\n",
    "    Args:\n",
    "        dataset : dataframe listing data and paths\n",
    "        n_sample : to work with a reduced dataset\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle dataset each epoch.\n",
    "        resize: Boolean,  if `True` resize images and mask to same dimensions\n",
    "        resize_width & resize_height: New dimensions after resizing\n",
    "        display: display images when calling dataloader\n",
    "        phase: allow to set if dataloader shall filter on train, val or test images/masks\n",
    "        augmentation: Variable defining augmentation of image and mask\n",
    "        normalization: Boolean,  if `True` normalizes images and masks RGB values ; The range for each individual colour is 0-255\n",
    "    \"\"\"\n",
    "    # We provide one dataset, containing labelIds and \n",
    "    def __init__(self, dataset, n_sample, batch_size=1, shuffle=False, resize=None, resize_width=256, resize_height=128, convert=None, display=None, phase='train', augmentation=None, normalization=None):\n",
    "        self.n_sample = n_sample\n",
    "        self.phase = phase\n",
    "        self.dataset = dataset.loc[dataset['Phase'] == self.phase, :].sample(self.n_sample) #ou dataset si on veut prendre tour le jeu de donnÃ©e\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.resize = resize\n",
    "        self.resize_width = resize_width\n",
    "        self.resize_height = resize_height\n",
    "        self.convert = convert\n",
    "        self.augmentation = augmentation\n",
    "        self.normalization = normalization\n",
    "        self.display = display\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    CATS = {\n",
    "        'void': [0, 1, 2, 3, 4, 5, 6],\n",
    "        'flat': [7, 8, 9, 10],\n",
    "        'construction': [11, 12, 13, 14, 15, 16],\n",
    "        'object': [17, 18, 19, 20],\n",
    "        'nature': [21, 22],\n",
    "        'sky': [23],\n",
    "        'human': [24, 25],\n",
    "        'vehicle': [26, 27, 28, 29, 30, 31, 32, 33,-1]\n",
    "    }\n",
    "    \n",
    "    def _convert_mask(self,img):\n",
    "        if len(img.shape) == 3:\n",
    "            img = np.squeeze(img[:, :, 0])\n",
    "        else:\n",
    "            img = np.squeeze(img)\n",
    "        mask = np.zeros((img.shape[0], img.shape[1], 8),dtype=np.uint8)\n",
    "        for i in range(-1, 34):\n",
    "            if i in self.CATS['void']:\n",
    "                mask[:,:,0] = np.logical_or(mask[:,:,0],(img==i))\n",
    "            elif i in self.CATS['flat']:\n",
    "                mask[:,:,1] = np.logical_or(mask[:,:,1],(img==i))\n",
    "            elif i in self.CATS['construction']:\n",
    "                mask[:,:,2] = np.logical_or(mask[:,:,2],(img==i))\n",
    "            elif i in self.CATS['object']:\n",
    "                mask[:,:,3] = np.logical_or(mask[:,:,3],(img==i))\n",
    "            elif i in self.CATS['nature']:\n",
    "                mask[:,:,4] = np.logical_or(mask[:,:,4],(img==i))\n",
    "            elif i in self.CATS['sky']:\n",
    "                mask[:,:,5] = np.logical_or(mask[:,:,5],(img==i))\n",
    "            elif i in self.CATS['human']:\n",
    "                mask[:,:,6] = np.logical_or(mask[:,:,6],(img==i))\n",
    "            elif i in self.CATS['vehicle']:\n",
    "                mask[:,:,7] = np.logical_or(mask[:,:,7],(img==i))\n",
    "        return np.array(np.argmax(mask,axis=2), dtype='uint8')\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # collect batch images and masks\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        #print('start barch is ', start)\n",
    "        #print('stop barch is ', stop)\n",
    "        images = []\n",
    "        masks = []\n",
    "        for j in range(start, stop):\n",
    "            # Store entire paths to image_file and mask_file variable\n",
    "            image_file = list(self.dataset['Path_x'])[j]\n",
    "            mask_file = list(self.dataset['Path_y'])[j]\n",
    "            unique_index_file = list(self.dataset['File_unique_index'])[j]\n",
    "\n",
    "            # OpenImages and masks using OpenCV ibrary, and convert in np array\n",
    "            image = cv2.imread(image_file)\n",
    "            #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            mask = cv2.imread(mask_file)\n",
    "            #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # apply resize\n",
    "            if self.resize:\n",
    "                dim = (self.resize_width, self.resize_height)\n",
    "                original_height = image.shape[0]\n",
    "                original_width = image.shape[1]\n",
    "                original_channels = image.shape[2]\n",
    "                image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA) # resize image\n",
    "                mask = cv2.resize(mask, dim, interpolation = cv2.INTER_AREA)\n",
    "                new_height = image.shape[0]\n",
    "                new_width = image.shape[1]\n",
    "                new_channels = image.shape[2]\n",
    "                \n",
    "            if self.convert:\n",
    "                mask = self._convert_mask(mask)\n",
    "                \n",
    "            # apply display\n",
    "            if self.display:\n",
    "                #Display Image and Mask\n",
    "                plt.figure(figsize=(25, 5))\n",
    "                plt.subplots_adjust(hspace=0.5)\n",
    "                plt.subplot(120 + 1 + 0)\n",
    "                plt.imshow(image)\n",
    "                plt.subplot(120 + 1 + 1)\n",
    "                plt.imshow(mask)\n",
    "                plt.grid(False)\n",
    "                plt.suptitle(f'Images and Masks {unique_index_file}')\n",
    "                plt.show()\n",
    "                if self.resize:\n",
    "                    print('original height was ', original_height, ' and new height is ', new_height)\n",
    "                    print('original width was ', original_width, ' and new width is ', new_width)\n",
    "                    print('original channels was ', original_channels, ' and new channels is ', new_channels)\n",
    "                    \n",
    "\n",
    "            # apply normalization : Normalizing RGB values ; The range for each individual colour is 0-255 (as 2^8 = 256 possibilities).\n",
    "            if self.normalization:\n",
    "                #normalizedImg = np.zeros((800, 800))\n",
    "                image = cv2.normalize(image, None, 0, 1, cv2.NORM_MINMAX) #np.array(image/255, dtype='uint8') \n",
    "                #mask = np.array(mask/255, dtype='uint8') # cv2.normalize(mask, None, 0, 1, cv2.NORM_MINMAX) #mask/255\n",
    "                # apply display\n",
    "                if self.display:\n",
    "                    print(f'----------------Normalized Image and Mask {j} from batch collected-----------------')\n",
    "                    #Display Image and Mask\n",
    "                    plt.figure(figsize=(25, 5))\n",
    "                    plt.subplots_adjust(hspace=0.5)\n",
    "                    plt.subplot(120 + 1 + 0)\n",
    "                    plt.imshow(image)\n",
    "                    plt.subplot(120 + 1 + 1)\n",
    "                    plt.imshow(mask)\n",
    "                    plt.grid(False)\n",
    "                    plt.suptitle(f'Normalized Images and Masks {unique_index_file}')\n",
    "                    plt.show()\n",
    "            \n",
    "            # apply augmentations\n",
    "            if ((self.augmentation is not None) & (self.phase == 'train')):\n",
    "                # Augment an image\n",
    "                sample = self.augmentation(image=image, mask=mask)\n",
    "                augmented_image, augmented_mask = sample['image'], sample['mask']\n",
    "                # apply display\n",
    "                if self.display:\n",
    "                    print(f'----------------Augmented Image and Mask {j} from batch collected-----------------')\n",
    "                    #Display Image and Mask\n",
    "                    plt.figure(figsize=(25, 5))\n",
    "                    plt.subplots_adjust(hspace=0.5)\n",
    "                    plt.subplot(120 + 1 + 0)\n",
    "                    plt.imshow(augmented_image)\n",
    "                    plt.subplot(120 + 1 + 1)\n",
    "                    plt.imshow(augmented_mask)\n",
    "                    plt.grid(False)\n",
    "                    plt.suptitle(f'Augmented Images and Masks {unique_index_file}')\n",
    "                    plt.show()\n",
    "                    images.append(augmented_image)\n",
    "                    masks.append(augmented_mask)\n",
    "                    \n",
    "            # append image and mask to batch\n",
    "            images.append(image)\n",
    "            masks.append(mask)   \n",
    "            print(f'---------------- Image and Mask {j} from batch collected in Data Generator-----------------')\n",
    "\n",
    "        # transpose list of lists\n",
    "        image_batch = np.stack(images, axis=0) #A confirmer si images ou image\n",
    "        mask_batch = np.stack(masks, axis=0) #A confirmer si masks ou mask\n",
    "\n",
    "        #Add 1 dimension to return image_batch and mask_batch in 4 dimensions (batch number, height, width, channels), avec channel = 1 pour les masques et =3 pour les images\n",
    "        if len(mask_batch.shape) == 3:\n",
    "            mask_batch = np.expand_dims(mask_batch, axis = 3)\n",
    "\n",
    "        #print('mask_batch shape is ', mask_batch.shape)\n",
    "        return image_batch, mask_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle dataset each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.dataset.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56464d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATALOADER - Notre classe hÃ©rite de la classe Keras.utils.Sequence\n",
    "# # Elle permet de crÃ©er un gÃ©nÃ©rateur de donnÃ©es\n",
    "# # Cette classe parente vous assure dans le cas ou vous souhaitez utiliser du calcul parallÃ¨le avec vos threads, de garantir de parcourir une seule et unique fois vos donnÃ©es au cours dâune Ã©poch\n",
    "# class GeneratorCitySpace(keras.utils.Sequence):\n",
    "        \n",
    "#     CATS = {\n",
    "#         'void': [0, 1, 2, 3, 4, 5, 6],\n",
    "#         'flat': [7, 8, 9, 10],\n",
    "#         'construction': [11, 12, 13, 14, 15, 16],\n",
    "#         'object': [17, 18, 19, 20],\n",
    "#         'nature': [21, 22],\n",
    "#         'sky': [23],\n",
    "#         'human': [24, 25],\n",
    "#         'vehicle': [26, 27, 28, 29, 30, 31, 32, 33,-1]\n",
    "#     }\n",
    "    \n",
    "#     def _convert_mask(self,img):\n",
    "#         img = np.squeeze(img)\n",
    "#         mask = np.zeros((img.shape[0], img.shape[1], 8),dtype=np.uint8)\n",
    "#         for i in range(-1, 34):\n",
    "#             if i in self.CATS['void']:\n",
    "#                 mask[:,:,0] = np.logical_or(mask[:,:,0],(img==i))\n",
    "#             elif i in self.CATS['flat']:\n",
    "#                 mask[:,:,1] = np.logical_or(mask[:,:,1],(img==i))\n",
    "#             elif i in self.CATS['construction']:\n",
    "#                 mask[:,:,2] = np.logical_or(mask[:,:,2],(img==i))\n",
    "#             elif i in self.CATS['object']:\n",
    "#                 mask[:,:,3] = np.logical_or(mask[:,:,3],(img==i))\n",
    "#             elif i in self.CATS['nature']:\n",
    "#                 mask[:,:,4] = np.logical_or(mask[:,:,4],(img==i))\n",
    "#             elif i in self.CATS['sky']:\n",
    "#                 mask[:,:,5] = np.logical_or(mask[:,:,5],(img==i))\n",
    "#             elif i in self.CATS['human']:\n",
    "#                 mask[:,:,6] = np.logical_or(mask[:,:,6],(img==i))\n",
    "#             elif i in self.CATS['vehicle']:\n",
    "#                 mask[:,:,7] = np.logical_or(mask[:,:,7],(img==i))\n",
    "#         return np.array(np.argmax(mask,axis=2), dtype='uint8')\n",
    "    \n",
    "#     def _transform_data(self,X,Y):\n",
    "#         if len(Y.shape) == 3:\n",
    "#             Y = np.expand_dims(Y, axis = 3)\n",
    "#         X = X /255. \n",
    "#         return np.array(X,dtype=np.uint8), Y\n",
    "    \n",
    "#     def __init__(self, image_filenames, labels, batch_size,crop_x,crop_y):\n",
    "#         \"\"\"GÃ©nÃ©rateur de donnÃ©es avec augmentation des images\n",
    "#         \"\"\"\n",
    "#         self.image_filenames, self.labels = image_filenames, labels\n",
    "#         self.batch_size = batch_size\n",
    "#         self.crop_x,self.crop_y = crop_x, crop_y\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         x=[cv2.resize(cv2.imread(path_X),(self.crop_x,self.crop_y)) for path_X in batch_x]\n",
    "#         y = [cv2.resize(self._convert_mask(cv2.imread(path_Y,0)),(self.crop_x,self.crop_y)) for path_Y in batch_y]\n",
    "#         y=np.array(y)\n",
    "#         x=np.array(x)\n",
    "#         return self._transform_data(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa678fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADER - Notre classe hÃ©rite de la classe Keras.utils.Sequence\n",
    "# Elle permet de crÃ©er un gÃ©nÃ©rateur de donnÃ©es\n",
    "# Cette classe parente vous assure dans le cas ou vous souhaitez utiliser du calcul parallÃ¨le avec vos threads, de garantir de parcourir une seule et unique fois vos donnÃ©es au cours dâune Ã©poch\n",
    "class GeneratorCitySpace(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset to build bacthes\n",
    "    Args:\n",
    "        dataset : dataframe listing data and paths\n",
    "        n_sample : to work with a reduced dataset\n",
    "        batch_size: Integet number of images in batch.\n",
    "        resize_width & resize_height: New dimensions after resizing\n",
    "        phase: allow to set if dataloader shall filter on train, val or test images/masks\n",
    "        augmentation: Variable defining augmentation of image and mask\n",
    "    \"\"\"\n",
    "    # We provide one dataset, containing labelIds Masks and Images\n",
    "    def __init__(self, dataset, n_sample=None, batch_size=1, resize_width=256, resize_height=128, phase='train', augmentation=None):\n",
    "        self.n_sample = n_sample\n",
    "        self.phase = phase\n",
    "        if (self.n_sample is None):\n",
    "            self.dataset = dataset.loc[dataset['Phase'] == self.phase, :]\n",
    "        else:\n",
    "            self.dataset = dataset.loc[dataset['Phase'] == self.phase, :].sample(self.n_sample) #ou dataset si on veut prendre tour le jeu de donnÃ©e\n",
    "        self.batch_size = batch_size\n",
    "        self.resize_width = resize_width\n",
    "        self.resize_height = resize_height\n",
    "        self.augmentation = augmentation\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    CATS = {\n",
    "        'void': [0, 1, 2, 3, 4, 5, 6],\n",
    "        'flat': [7, 8, 9, 10],\n",
    "        'construction': [11, 12, 13, 14, 15, 16],\n",
    "        'object': [17, 18, 19, 20],\n",
    "        'nature': [21, 22],\n",
    "        'sky': [23],\n",
    "        'human': [24, 25],\n",
    "        'vehicle': [26, 27, 28, 29, 30, 31, 32, 33,-1]\n",
    "    }\n",
    "    \n",
    "    def _convert_mask(self,img):\n",
    "        img = np.squeeze(img)\n",
    "        mask = np.zeros((img.shape[0], img.shape[1], 8),dtype=np.uint8)\n",
    "        for i in range(-1, 34):\n",
    "            if i in self.CATS['void']:\n",
    "                mask[:,:,0] = np.logical_or(mask[:,:,0],(img==i))\n",
    "            elif i in self.CATS['flat']:\n",
    "                mask[:,:,1] = np.logical_or(mask[:,:,1],(img==i))\n",
    "            elif i in self.CATS['construction']:\n",
    "                mask[:,:,2] = np.logical_or(mask[:,:,2],(img==i))\n",
    "            elif i in self.CATS['object']:\n",
    "                mask[:,:,3] = np.logical_or(mask[:,:,3],(img==i))\n",
    "            elif i in self.CATS['nature']:\n",
    "                mask[:,:,4] = np.logical_or(mask[:,:,4],(img==i))\n",
    "            elif i in self.CATS['sky']:\n",
    "                mask[:,:,5] = np.logical_or(mask[:,:,5],(img==i))\n",
    "            elif i in self.CATS['human']:\n",
    "                mask[:,:,6] = np.logical_or(mask[:,:,6],(img==i))\n",
    "            elif i in self.CATS['vehicle']:\n",
    "                mask[:,:,7] = np.logical_or(mask[:,:,7],(img==i))\n",
    "        return np.array(mask,dtype='uint8') #retrun the mask OneHotEncoded\n",
    "        #return np.array(np.argmax(mask,axis=2), dtype='uint8') #return the mask with the labelId\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # collect batch images and masks\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        images = []\n",
    "        masks = []\n",
    "        for j in range(start, stop):\n",
    "            # OpenImages and masks, and apply resize + mask color conversion + Normalization\n",
    "            image = (cv2.resize(cv2.imread(list(self.dataset['Path_x'])[j]), (self.resize_width, self.resize_height), interpolation = cv2.INTER_AREA)) #image/255 ou normalizedImg\n",
    "            mask = self._convert_mask(cv2.resize(cv2.imread(list(self.dataset['Path_y'])[j],0), (self.resize_width, self.resize_height), interpolation = cv2.INTER_AREA)) #mask/255\n",
    "            #mask = cv2.resize(cv2.imread(list(self.dataset['Path_y'])[j]), (self.resize_width, self.resize_height), interpolation = cv2.INTER_AREA) #mask/255\n",
    "            \n",
    "            if ((self.augmentation is not None) & (self.phase == 'train')):\n",
    "                # apply augmentations\n",
    "                sample = self.augmentation(image=image, mask=mask)\n",
    "                augmented_image, augmented_mask = sample['image'], sample['mask'] \n",
    "                #augmented_image = augmented_image/255\n",
    "                #augmented_mask = augmented_mask/255\n",
    "                # append augmented image and mask to batch\n",
    "                images.append(augmented_image)\n",
    "                masks.append(augmented_mask)\n",
    "            image = image/255\n",
    "            #mask = mask/255\n",
    "\n",
    "\n",
    "            # append image and mask to batch\n",
    "            images.append(image)\n",
    "            masks.append(mask)   \n",
    "\n",
    "        # transpose list of lists = Data Generator outputs\n",
    "        image_batch = np.stack(images, axis=0) \n",
    "        #Add 1 dimension to return image_batch and mask_batch in 4 dimensions (batch number, height, width, channels), avec channel = 1 pour les masques et =3 pour les images\n",
    "        #mask_batch = np.expand_dims(np.stack(masks, axis=0), axis = 3)\n",
    "        mask_batch = np.stack(masks, axis=0) \n",
    "\n",
    "        # Data Generator outputs\n",
    "        return image_batch, mask_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.dataset) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle dataset each epoch\"\"\"\n",
    "        np.random.shuffle(self.dataset.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034902d",
   "metadata": {},
   "source": [
    "# Model UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aaea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "\n",
    "#Encoder block: Double convolution avec maxpooling\n",
    "#Durant cette phase, la taille de l'image rÃ©duit graduellement, alors que la profondeur s'accroit.\n",
    "#Cela signifie que le rÃ©seau apprend le \"QUOI\", mais perd le \"OÃ¹\"\n",
    "def encoder_block(input, num_filters, maxpool=None, dropout=None):\n",
    "    x = Conv2D(num_filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input)\n",
    "    x = Conv2D(num_filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "    if dropout:\n",
    "        d = Dropout(0.5)(x)\n",
    "        if maxpool:\n",
    "            p = MaxPooling2D(pool_size=(2, 2))(d)    \n",
    "            return x, p, d\n",
    "        else:\n",
    "            return x, d\n",
    "    if maxpool:\n",
    "        p = MaxPooling2D(pool_size=(2, 2))(x)    \n",
    "        return x, p\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "#Decoder block\n",
    "#Durant cette phase, le dÃ©coder accroit la taille de l'image, alors que la prodondeur rÃ©duit\n",
    "#Il retrouve l'information \"OÃ¹\" avec les Up_Sampling. \n",
    "#Au travers de la concatÃ©nation, i les Skip-Connections des couches.\n",
    "# #A la suite de chaque concatÃ©nation, nous appliquons encore les coubles convolutions\n",
    "def decoder_block(input, skip_connect, num_filters):\n",
    "    x = Conv2D(num_filters, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(input))\n",
    "    x = concatenate([skip_connect, x], axis = 3) #skip connections that concatenate the encoder feature map with the decoder\n",
    "    x = Conv2D(num_filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "    x = Conv2D(num_filters, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "    return x    \n",
    "\n",
    "    #Build Unet using the blocks\n",
    "def build_unet_block(input_shape, nb_class, n_filters = 32):\n",
    "    inputs = Input(input_shape) #(resize_height, resize_width, 3) #Transposed compare to input dim of data generator\n",
    " \n",
    "    conv1, pool1 = encoder_block(inputs, n_filters * 1, maxpool=True, dropout=None)\n",
    "    conv2, pool2 = encoder_block(pool1, n_filters * 2, maxpool=True, dropout=None)\n",
    "    conv3, pool3 = encoder_block(pool2, n_filters * 4, maxpool=True, dropout=None)\n",
    "    conv4, pool4, drop4 = encoder_block(pool3, n_filters * 8, maxpool=True, dropout=True)\n",
    "    conv5, drop5 = encoder_block(pool4, n_filters * 16, maxpool=None, dropout=True)\n",
    "\n",
    "    conv6 = decoder_block(drop5, drop4, n_filters * 8)\n",
    "    conv7 = decoder_block(conv6, conv3, n_filters * 4)\n",
    "    conv8 = decoder_block(conv7, conv2, n_filters * 2)\n",
    "    conv9 = decoder_block(conv8, conv1, n_filters * 1)\n",
    "    \n",
    "    outputs = Conv2D(nb_class, 1, activation = 'softmax')(conv9)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs], name='U-Net')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c706d8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\Users\\blanc\\anaconda3\\envs\\IA_Poject8\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "def build_unet():\n",
    "    inputs = Input((resize_width, resize_height, 3))\n",
    "    # Input\n",
    "    s = Lambda(x)(inputs)\n",
    "    # Layer 1 \n",
    "    c1 = Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(32, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPool2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(64, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPool2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(128, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPool2D((2, 2))(c3)\n",
    "\n",
    "\n",
    "    c4 = Conv2D(256, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(256, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPool2D((2, 2))(c4)\n",
    "\n",
    "\n",
    "    c5 = Conv2D(512, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(512, (3,3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "\n",
    "\n",
    "\n",
    "    u7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "\n",
    "\n",
    "\n",
    "    u8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "\n",
    "\n",
    "\n",
    "    u9 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    c9 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "\n",
    "    outputs = Conv2D(8, (1, 1), activation='softmax')(c9)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[outputs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14cea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "  \n",
    "def get_unet(input_img, n_filters = 64, dropout = 0.1, batchnorm = True):\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(8, (1, 1), activation='softmax')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774bc51",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('IA_Project8_tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f1d47f269b93e41448a34bb5a1a0228ca81825d68373673cfd6b3768418f56c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
